{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "* [Outline](../0_Introduction/0_introduction.ipynb)\n",
    "* [Glossary](../0_Introduction/1_glossary.ipynb)\n",
    "* [5. Imaging](5_0_introduction.ipynb)\n",
    "    * Previous: [5.2 Sampling functions and PSFs](5_2_sampling_functions_and_psfs.ipynb) \n",
    "    * Next: [5.4 The Dirty Image and Visibility Weights](5_4_imaging_weights.ipynb)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import section specific modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import track_simulator\n",
    "import AA_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.3 Gridding and Degridding for using the FFT <a id='imaging:sec:gridding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section several sampling functions were presented. There the sampling functions were already neatly discretized into images, where each image was a grid of pixels (all with the same size). Fourier inverting such regularly sampled data is done with the Fast Fourier Transform (FFT) algorithm, which operate in roughly $2N^2\\log(N)$ steps instead of the normal $N^2M$ with $M\\approx N^2$ complex exponentiations and multiplies that would be required in a direct approach, which calculates the intensity of each pixel:\n",
    "\n",
    "\\begin{equation}\n",
    " I(l,m) = \\sum_{k=0}^{M-1}V_k(u,v)e^{2\\pi i (lu+mv)}\\text{, }V_k\\text{ are the M measurements taken by the telescope}\n",
    "\\end{equation}\n",
    "\n",
    "From this it should be clear that as the number of baselines or observation time is increased, the FFT approach would be far less time-consuming than the the direct approach. Unfortunately radio interferometers don't generally take measurements at regular intervals, and thus an FFT cannot be used on the observation data directly. Instead the data has to be *resampled* onto a grid with points spaced at regular intervals before taking the FFT. This resampling process and its inverse is the topic of this section.\n",
    "\n",
    "A second problem related to using the FFT transform approach is that of aliasing. The FFT assumes that the input signal (here the spacial frequency domain) is periodic in nature. The resultant image constructed by resampling and inverse FFT therefore repeats at regular intervals: sources near the top of the image is aliased back into the image at the bottom for instance. This introduces the necessity to filter the image with a filter that only passes signal that falls within the field of view being reconstructed. An example of this aliasing will be given later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Image Resolution and Pixel Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating an image from visibilities, using either a direct or fast Fourier transform, two parameters need to be defined: the resolution of each pixel and the extent of the image either as a number of pixels or as the size of the field of view (depending on the particular imager). An image will be a 2-d array of size $N_l \\times N_m$ and each pixel will have a resolution of $(\\Delta \\theta_l, \\Delta \\theta_m)$ if we are making the small angle approximation for the field of view. Recall that the image size is $l = \\cos{\\theta_l}$, $m = \\cos{\\theta_m}$, the resolution is $\\Delta l = \\cos{\\Delta \\theta_l}$, $\\Delta m = \\cos{\\Delta \\theta_m}$ and in the small angle approximation $\\Delta l \\sim \\Delta \\theta_l$, $\\Delta m \\sim \\Delta \\theta_m$. Though, many imagers can create images which break the small angle approximation the notation is retained. There are a number of techniques for representing a spherical coordinates, via a non-linear transform, on a 2-d plane. In radio interferometry the standard technique is SIN-projection, see [<cite data-cite='Greisen1994'>AIPS Memo 27</cite> &#10548;](ftp://ftp.aoc.nrao.edu/pub/software/aips/TEXT/PUBL/AIPSMEMO27.PS) for a detailed discussion of different coordinate projections.\n",
    "\n",
    "Given the the resolution $(\\Delta \\theta_l, \\Delta \\theta_m)$ and the desired field of view $(\\theta_l, \\theta_m)$ the number of pixels in the image is\n",
    "\n",
    "$$N_l = \\frac{\\theta_l}{\\Delta \\theta_l}$$\n",
    "\n",
    "$$N_m = \\frac{\\theta_m}{\\Delta \\theta_m}$$\n",
    "\n",
    "For a given image resolution and size the uv domain resolution/grid size $(\\Delta u, \\Delta v)$ is\n",
    "\n",
    "$$\\Delta u = \\frac{1}{N_l \\Delta \\theta_l}$$\n",
    "\n",
    "$$\\Delta v = \\frac{1}{N_m \\Delta \\theta_m}$$\n",
    "\n",
    "And the number of pixels is unchanged $N_u = N_l$, $N_v = N_m$.\n",
    "\n",
    "An important note about the number of pixels: try to use a value of which is a power of 2, i.e. $N_l = 2^j$, $N_m = 2^k$. This is because of how FFT's are implemented, the optimal run-time efficiency of an FFT is with input lenghts which are powers of 2 and are least efficient when the input length is a prime number. For example, the time required to generate a 256 by 256 pixel image will be less than a 251 by 251 pixel image even though the resulting image will have more pixels because $256 = 2^8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 The UV plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off the discussion on the resampling and image synthesis process let's consider a simulated JVLA observation. The JVLA has 27 reconfigurable antennae. We will use the compact D configuration as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Globals file with JVLA D-configuration antenna ENU coordinates\n",
    "%run jvla_d_constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"ENU ANTENNA COORDINATES\")\n",
    "plt.plot(ENU[:,0],ENU[:,1],\"r+\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Here the positions of the 27 antennae of the JVLA are shown (compact D-configuration)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section in order to accurately reconstruct an image it is necessary to measure a significant portion of the spacial frequency domain. Luckily good coverage on the u,v plane (plane on which the measurements are taken) don't require physically moving the antennae while observing. The Fourier relationship between the sky and the measurements taken by an interferometer depends on the orientation of the baseline vectors between antennae pairs relative to the sky, and these changes as the Earth rotates. As the earth rotates more samples are taken and the tracks swept out by each antenna pair increases in length. As the u,v coordinates are scaled by wavelength the individual tracks can be broadened by integrating many consequtive channels together. Both factors contribute to the accuracy of the reconstruction. The following plots show how the u,v coverage (swept out by pairs of antennae) depend on declination angle, as well as the observation length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uv_5min = track_simulator.sim_uv(0.0,90.0,5/60.0,60/3600.0,ENU,ARRAY_LATITUDE,True)\n",
    "uv_6hrs_90_dec = track_simulator.sim_uv(0.0,90.0,6,60/3600.0,ENU,ARRAY_LATITUDE,True)\n",
    "uv_6hrs_45_dec = track_simulator.sim_uv(0.0,45.0,6,60/3600.0,ENU,ARRAY_LATITUDE,True)\n",
    "uv_6hrs_20_dec = track_simulator.sim_uv(0.0,20.0,6,60/3600.0,ENU,ARRAY_LATITUDE,True)\n",
    "uv_6hrs_0_dec = track_simulator.sim_uv(0.0,0.0,6,60/3600.0,ENU,ARRAY_LATITUDE,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: UV coverage at various declinations. The projection of the baselines onto the UV plane at $90^\\circ$ are concentric circles. As the observation declination decreases the projections become straight lines. Here the baselines and their conjugates are plotted. An interferometer measures both the visibility and its complex conjugate simultaniously (baseline and its negative). The conjugate baselines are redundant measurements and can be discarded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as is evident these samples do not fall at regularly spaced positions. The goal of *gridding* is to \"interpolate\" these samples onto regularly spaced grid positions. As you will see later some u,v space image deconvolution algorithms such as the Cotton-Schwab major-minor cycle algorithm requires that sources in image space are reconverted back into the non-regular measurement space. Here an accurate *degridding* operation is required to \"interpolate\" regularly sampled visibilities back onto the u,v tracks shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Gridding and Degridding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may suspect there are many ways to interpolate data to and from regular coordinates. The most widely-used technique used in radio imaging programs, such as lwimager, is known as \"convolutional-resampling\". In this technique each visibility is weighted and smeared out onto grid points that lie within a small distance from the original coordinate.\n",
    "\n",
    "The value at each grid point is a weighted accumulation of all the nearby visibilties. In one dimension this can be stated as:\n",
    "\\begin{equation}\n",
    " (\\forall a \\in \\mathbb{Z}) f(a\\Delta{x}) = \\sum_{k\\in Q\\subset\\mathbb{R}}f(k)C(a\\Delta{x}-k)\n",
    "\\end{equation}\n",
    "\n",
    "The weighting function, $C$ can be any of the miriad of functions proposed in the literature. These include linear, Lagrange, sinc (including one of the many windowing functions), Gaussian, modified B-spline, etc. \n",
    "\n",
    "You may have noticed that the interpolating function above is remarkably close to that of a discrete convolution. If the resampling was done on data that was regularly sampled and the convolution function evaluated at these regular discrete steps then the function would just the ordinary discrete convolution. However, the function as it stands is not quite a convolution, neither is it interpolation by the strictest definition of the word. Gridding and degridding should be thought of as *non-discrete integral convolution approximations*, but we will use the regular convolution notation for it in our discussion.\n",
    "\n",
    "It is useful to think of these non-discrete convolution operations in terms of the ordinary up- and downsampling operations. In gridding, as with traditional upsampling the space in-between samples are filled with zero values. The only difference is that with gridding the original measurements are not regularly spaced, as would be the case with upsampling. Just as with ordinary upsampling it is then necessary to assign values to these new zero values in-between the measured values. With gridding the values are smeared out over the grid points within a some area of support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"figures/gridding_illustration.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Each observed visibility (per channel and baseline) is centered at some precomputed coordinate\n",
    "in continuous u,v space and is “convolved” with some function $C(u,v)$, which extends only to finite “full support”\n",
    "region as illustrated. The result is binned in a regularly spaced grid when gridding or gathered from this grid when degridding. It should be clear that this is not the standard convolution operator or interpolation. After all the observed visibilities have been gridded an Inverse Fast Fourier Transform is performed and a correcting function is applied to construct an image. The reverse operations are done when simulating a set of visibility measurements from a model sky.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this understanding in hand we can define gridding and degridding more rigorously:\n",
    "\\begin{equation}\n",
    " \\begin{split}\n",
    "     V_\\text{gridded}[u,v]&=[(\\mathscr{V}(u,v)S(u,v))\\circ C(u,v)]III[u,v]\\\\\n",
    "     V_\\text{degridded}(u,v)&=[V_\\text{gridded}[u,v]\\circ C(u,v)]S(u,v)\\\\\n",
    " \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "In gridding the sampled visibilities are convolved with a convolution function then discretized onto regular points by the shah (bed-of-nails) function. In degridding the opposite is done: the regularly sampled discerete values are convolved and sampled along the sampling tracks in the u,v plane.\n",
    "\n",
    "The convolution function, $C$, smears (gridding) and gathers (degridding) the visibilities over / from some area of support before discretizing the visibilities to new coordinates. Ideally this function would be computed during the gridding and degridding operations, however, considering that the processing costs of gridding and degridding both scale as $MC_\\text{sup}^2$ these functions can be too computationally expensive to compute for every visibility and is normally pretabulated for a given support size. Additionally it is important to sample this function much more densely than the spacings between grid cells; interferometers take measurements in the spatial frequency domain and any large snapping / rounding operation on the coordinates of the samples will result in a decorrelation in the structural information about the image. The figure below illustrates how values are picked from the oversampled filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"figures/oversampled_filter_illustration.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: A ficticious filter is shown here. Here the padded and oversampled C[x] filter is illustrated for a 3-cell full-support region (half support of 1 to both sides of the centre value), padded with one value on both sides. The filter is 5x oversampled, as indicated by the spaces between the asterisks. The bars represent the cell-spacing ($\\Delta{u}$ for instance) used for the grid. If the measured uv coordinate falls exactly on the nearest grid cell then values 6,11 and 16 are selected as interpolation coefficients. If $\\text{round}(\\text{fraction}(u, v)m_\\text{oversample factor})$ = 2 for instance then 8, 13 and 18 are selected for the 3 grid points being “convolved”. In other words: a denser bed of nails is placed over the bed of nails of the grid and the closest set of coefficients for the convolution are selected.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to accuracy the alias-reduction properties of the convolution filter, $C$ being used is essential to the FFT approach. By the convolution relation the reconstructed image of the radio sky can be stated as follows:\n",
    "\\begin{equation}\n",
    "I_\\text{dirty}[l,m] = ([I(l,m)\\circ\\text{PSF}(l,m)]c(l,m))\\circ\\mathscr{F}\\{III(u,v)\\}(l,m)\n",
    "\\end{equation}\n",
    "\n",
    "Convolution with the fourier transform of the shah function (a series of periodic functions in the image domain) replicates the field of view at a period of $M\\text{cell}_l$ and $N\\text{cell}_m$ for an $M\\times N$ pixel image, and it is this undesirable replication that must be stopped. To that end one would hope that the fourier transform of the convolution filter, $c(l,m)$, maximizes the following ratio:\n",
    "\\begin{equation}\n",
    "\\frac{\\int_\\text{FOV}|c(l,m)|^2dS}{\\int_{-\\infty}^\\infty|c(l,m)|^2dS}\n",
    "\\end{equation}\n",
    "\n",
    "Simply stated it is desirable that the function $c$ is only non-zero over a small central region, the field of view.\n",
    "\n",
    "Both the remarks about accuracy and fourier response precludes using a nearest neighbour approach to interpolating points to and from regular coordinates. This technique (also known as *cell-summing*) simply accumulates the neighbouring points that fall within a rectangular region around the new coordinate, without considering the distance those points are from the new coordinate. The fourier-response to this box function is an infinite sinc function, which ripples out slowly towards infinity and doesn't stop much of the aliasing energy. Convolutional gridding/degridding is therefore a more attractive approach, because the distance between the grid point and the measured uv point is taken into account when selecting a set of convolution weights.\n",
    "\n",
    "The observation about the fourier response of the box function leads us to a partial solution for the aliasing problem, in that the fourier reponse to convolving with an *infinite* sinc will yield a box response. Unfortunately this is not computationally feasible and instead the best option is to convolve with a windowed sinc function, or some other function that has a similar centre-heavy fourier response and preferably tapers off reasonably quickly. The images below illustrates the significant improvement using a truncated sinc function compared to nearest-neighbour interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"figures/NN_interpolation_aliasing.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename=\"figures/AA_kernel_alias_reduction.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Above two synthesized images are shown. The first using cell-summing (nearest neighbour) interpolation and the second using convolutional resampling with a simple truncated sinc function. In the first the sources of this grid sky pattern that fall slightly outside the field of view are aliased back into the field of view. In the second the aliasing energy is limited by the box response of the sinc function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the magnitude of the sidelobes of the fourier responses are plotted. The response of the box function is significantly higher than that of truncated and windowed sinc functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "half_sup = 6\n",
    "oversample = 15\n",
    "full_sup_wo_padding = (half_sup * 2 + 1)\n",
    "full_sup = full_sup_wo_padding + 2 #+ padding\n",
    "no_taps = full_sup + (full_sup - 1) * (oversample - 1)\n",
    "taps = np.arange(-no_taps//2,no_taps//2 + 1)/float(oversample)\n",
    "\n",
    "#unit box\n",
    "box = np.where((taps >= -0.5) & (taps <= 0.5),\n",
    "               np.ones([len(taps)]),np.zeros([len(taps)]))\n",
    "fft_box=np.abs(np.fft.fftshift(np.fft.fft(np.fft.ifftshift(box))))\n",
    "#truncated (boxed) sinc\n",
    "sinc = np.sinc(taps)\n",
    "fft_sinc=np.abs(np.fft.fftshift(np.fft.fft(np.fft.ifftshift(sinc))))\n",
    "#gaussian sinc\n",
    "alpha_1=1.55\n",
    "alpha_2=2.52\n",
    "gsinc = np.sin(np.pi/alpha_1*(taps+0.00000000001))/(np.pi*(taps+0.00000000001))*np.exp(-(taps/alpha_2)**2)\n",
    "fft_gsinc=np.abs(np.fft.fftshift(np.fft.fft(np.fft.ifftshift(gsinc))))\n",
    "#plot it up\n",
    "plt.figure(figsize=(5, 7), dpi=80)\n",
    "l = np.arange(-(no_taps)//2,(no_taps)//2+1) * (1.0/oversample)\n",
    "a, = plt.plot(2*l,fft_box)\n",
    "b, = plt.plot(2*l,fft_sinc)\n",
    "c, = plt.plot(2*l,fft_gsinc)\n",
    "ax = plt.gca()\n",
    "ax.set_xlim(0,no_taps//2 * (1.0/oversample))\n",
    "ax.set_yscale(\"log\", nonposy='clip')\n",
    "plt.legend([a,b,c],[\"Box\",\"Sinc\",\"Gaussian Sinc\"])\n",
    "plt.xlabel(\"$2\\Delta{u}l$\")\n",
    "plt.ylabel(\"Magnitude of $c(l)$\")\n",
    "plt.title(\"Magnitude of Fourier transforms of several convolution functions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: The magnitudes of the fourier transforms of various functions. It is desirable that most of the energy of these functions fall within some central region and that the response drops off sharply at the edge of this central region*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fourier transformation the effects of the convolution function on the image can be mitigated by point-wise dividing the image through by the fourier transform of the convolution function, $c(l,m)$. This has the effect of flattening the response of the passband, by removing the tapering towards the edges of the image, but raises the amplitude of any aliased sources at the edge of the image.\n",
    "\n",
    "In practice the proloid spheroidal functions are used in imaging programs such as lwimager, but the definition of these functions are beyond the scope of the introductory discussion here and the reader is referred to the work of Donald Rhodes, [<cite data-cite='rhodes1970spheroidal'>On the Spheroidal Functions</cite> &#10548;](http://cdm16009.contentdm.oclc.org/cdm/compoundobject/collection/p13011coll6/id/61576/rec/50) for a detailed discussion of their definition and proof of their aliasing reduction properties.\n",
    "\n",
    "It is also worth noting that the convolution functions used in gridding and degridding need not be the same function. In degridding the focus is solidly on the accuracy of the predicted visibility. Here it can be advantageous to minimize the difference between a direct transformation approach and a Fast Fourier Transform approach with degridding, see for instance the discussion by Sze Tan, [<cite data-cite='tan1986aperture'>Aperture-synthesis mapping and parameter estimation</cite> &#10548;](http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.384529) for further detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.4 Example simulator and imager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude this section with some sample code to illustrate prediction and imaging using resampling and the FFT. To start off let's set up a model sky with 50 random sources with a maximum brightness of 5Jy. Then we simulate the measurement space by taking the fourier transform of this model sky:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_sky=np.zeros([256,256])\n",
    "for i in range(50):\n",
    "    model_sky[int(np.random.rand()*255),int(np.random.rand()*255)] = np.random.rand()*5.0 \n",
    "\n",
    "model_regular = np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(model_sky))) #numpy's FFT doesn't need normalization\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Model sky\")\n",
    "plt.imshow(model_sky,cmap=\"gray\")\n",
    "plt.xlabel(\"l\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.subplot(132)\n",
    "plt.title(\"Amplitudes of measurement space\")\n",
    "plt.imshow(10*np.log10(np.abs(model_regular+0.000000000001)))\n",
    "plt.xlabel(\"u\")\n",
    "plt.ylabel(\"v\")\n",
    "plt.subplot(133)\n",
    "plt.title(\"Phase of measurement space\")\n",
    "plt.imshow(np.angle(model_regular))\n",
    "plt.xlabel(\"u\")\n",
    "plt.ylabel(\"v\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: The simulated model sky (l,m space) and its fourier transform in the measurement space (u,v space)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up a sampling pattern and scale the u,v tracks to ensure that they all fall within the u,v grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set up interferometer sampling pattern\n",
    "uvw = track_simulator.sim_uv(0.0,90.0,1.5,60/3600.0,ENU,ARRAY_LATITUDE)\n",
    "#Then scale the u,v coordinates such that the uv coverage falls entirely on the uv grid\n",
    "#Note: this scales the effective FOV by the similarity theorem\n",
    "max_u = np.max(np.abs(uvw[:,0]))\n",
    "max_v = np.max(np.abs(uvw[:,1]))\n",
    "print \"Maximum u,v: (%f,%f)\" % (max_u,max_v)\n",
    "#N * cell_size_in_rads = 2 * max_uv\n",
    "#therefore cell_size_in_rads = 2 * max_uv / N\n",
    "Nx = model_sky.shape[0]\n",
    "Ny = model_sky.shape[1]\n",
    "cell_size_u = 2 * max_u / (Nx)\n",
    "cell_size_v =  2 * max_v / (Ny)\n",
    "print \"Nyquest cell size (radians) in image space (%f,%f)\" % (cell_size_u,cell_size_v)\n",
    "scaled_uv = np.copy(uvw[:,0:2])\n",
    "scaled_uv[:,0] /= cell_size_u\n",
    "scaled_uv[:,1] /= cell_size_v\n",
    "scaled_uv[:,0] += Nx/2\n",
    "scaled_uv[:,1] += Ny/2\n",
    "print np.max(scaled_uv[:,0]),np.max(scaled_uv[:,1])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"Sampling of the measurement space (amplitude)\")\n",
    "plt.imshow(10*np.log10(0.00000000001+np.abs(model_regular)))\n",
    "plt.plot(scaled_uv[:,0],scaled_uv[:,1],\"w.\",label=\"Baselines\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: UV tracks scaled onto the regularly sampled measurement space*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the prediction (also known as the \"forward\" step) the measurements are resampled onto the u,v tracks of the interferometer using the *degridding* algorithm discussed above. Measurements are gathered and weighted from the vacinity of each of the points along the sampling track in order to \"predict\" a value at the u,v coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load convolutional_degridder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tabulated_filter = AA_filter.AA_filter(3,30,\"sinc\")\n",
    "vis = fft_degrid(model_sky,scaled_uv,Nx,Ny,tabulated_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes a simplified imaging step using the FFT and gridding. The visibilities on the irregularly-spaced u,v tracks are resampled onto regular coordinates. This is done by weighting and smearing each measured visibility out onto the regular coordinates in the vacinity of its u,v coordinate. After resampling the inverse FFT is used to transform the measurements in the spacial frequency domain to those in the spacial domain, thereby approximately reconstructing the model sky we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load convolutional_gridder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tabulated_filter = AA_filter.AA_filter(3,63,\"sinc\")\n",
    "dirty_sky, psf = grid_ifft(vis,scaled_uv,Nx,Ny,tabulated_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot it up :-)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Model sky\")\n",
    "plt.imshow(model_sky,cmap=\"gray\")\n",
    "plt.xlabel(\"l\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.subplot(132)\n",
    "plt.title(\"PSF\")\n",
    "plt.imshow(np.abs(psf),cmap=\"gray\")\n",
    "plt.xlabel(\"l\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.subplot(133)\n",
    "plt.title(\"Dirty map\")\n",
    "plt.imshow(np.abs(dirty_sky),cmap=\"gray\")\n",
    "plt.xlabel(\"l\")\n",
    "plt.ylabel(\"m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure: Reconstructed image of the model sky. This \"dirty\" image is convolved with the the PSF shown in the centre figure. Some of the fainter sources are hardly visible because of the sidelobes introduced by this convolution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this reconstruction can only ever be approximate, since the u,v plane is only partially sampled. The brightest sources are still visible, provided the observation time is long enough. Each of the sources are convolved with the PSF shown above in the centre figure; the ring-like structure of the PSF is clearly visible around the bright sources and can, in the worst case, obscure some of the fainter sources in the image. The deconvolution strategies discussed later attepts to remove the psf structure from the images and improves the fidelity of the reconstructed images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "* Next: [5.4 The Dirty Image and Visibility Weights](5_4_imaging_weights.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
